---
title: "Documentation"
permalink: /docs/
sidebar:
  title: "Home"
  nav: sidebar-docs  # See /docs/_data/navigation.yml
---

In this page, you can find documentation about the project.

## Installation

To install the project, you can use pip:

```bash
pip install weblinx
```

## Prerequisites

The dataset is quite large, so you might only be interested in a subset of the data. Thus, the easiest way to download the dataset is the use the `huggingface_hub` library:

```bash
# 1. Install the library
pip install huggingface_hub[cli]
# 2. Request access to the dataset on Huggingface using your account
# 3. Log in to Huggingface
huggingface-cli login
```

Now, you can download the dataset:

```python
from huggingface_hub import snapshot_download

# Other options: allow_patterns = ["*.json", "*.html", "*.png", "*.mp4"]
allow_patterns = ["*.json"]

# Download a subset of the dataset, or...
snapshot_download(repo_id="McGill-NLP/WebLINX", allow_patterns=allow_patterns, repo_type="dataset", local_dir="./data/webtasks")
# ... download the entire dataset
snapshot_download(repo_id="McGill-NLP/WebLINX", repo_type="dataset", local_dir="./data/webtasks")
```

## Quickstart

Now that you have the dataset, you can use the `weblinx` library to process it:

```python
from pathlib import Path
import weblinx as wl


save_dir = Path("./data/webtasks")
split_path = save_dir / "splits.json"

# Load the name of the demonstrations in the training split
demo_names = wl.utils.load_demo_names_in_split(split_path, split='train')  # or 'valid' or 'test-iid'

# Load the demonstrations
demos = [wl.Demonstration(name) for name in names]

# Select a demo to work with
demo = demos[0]

# Load the Replay object, which contains the turns of the demonstration
replay = wl.Replay.from_demonstration(demo)

# Filter the turns to keep only the ones that are relevant for the task
turns = replay.filter_by_intents(
    "click", "textInput", "load", "say", "submit"
)

# Only keep the turns that have a good screenshot (i.e., the screenshot is not empty)
turns = filter_turns(
    turns, lambda t: t.has_screenshot() and t.get_screenshot_status() == "good"
)

# Remove chat turns where the speaker is not the navigator (e.g. if you want to train a model to predict the next action)
turns = filter_turns(
    turns,
    lambda turn: not (
        turn.type == "chat" and turn.get("speaker") != "navigator"
    ),
)
```

As you can see, the core `weblinx` has many useful functions to process the dataset, as well as various useful classes to represent the data (e.g. `Demonstration`, `Replay`, `Turn`). You can find more information about the library in the [documentation of the core WebLINX module](/docs/core/).

You can also take a look at some useful `processing` functions in the [documentation of the processing module](/docs/processing/), and some useful `utils` functions in the [documentation of the utils module](/docs/utils/).

## Examples for training

Here's an example of how to use the `weblinx` library to build input records for training the model:

```python
from pathlib import Path
import weblinx as wl


save_dir = Path("./data/webtasks")
split_path = save_dir / "splits.json"

# Load the name of the demonstrations in the training split
demo_names = wl.utils.load_demo_names_in_split(split_path, split='train')  # or 'valid' or 'test-iid'

# Load the demonstrations
demos = [wl.Demonstration(name) for name in names]

# Load candidates generated by DMR
candidates = load_candidate_elements(path='path/to/candidates.json')

def build_prompt_records_for_llama(*args, **kwargs):
    # Your code here
    pass

# Select turns and candidates for prompting from the demonstrations
selected_turns = select_turns_and_candidates_for_prompts(
    demos=demos,
    candidates=candidates,
    num_candidates=10,
)

# Build input records for training the model
input_records = build_input_records_from_selected_turns(
    selected_turns=selected_turns,
    format_intent=format_intent,
    build_prompt_records_fn=build_prompt_records_fn,
    format_prompt_records_fn=format_prompt_llama,
)
```